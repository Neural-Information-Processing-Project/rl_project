Logging to /tmp/openai-2022-06-15-15-34-54-459852
------------
{'lr': 0.0003, 'replay_size': 500000, 'ptau': 0.005, 'gamma': 0.99, 'burn_in': 10000, 'total_timesteps': 5000000.0, 'expl_noise': 0.2, 'batch_size': 256, 'policy_noise': 0.3, 'noise_clip': 0.5, 'policy_freq': 3, 'hidden_sizes': [300, 300], 'env_name': 'humanoid-dir', 'seed': 0, 'alg_name': 'mql', 'disable_cuda': False, 'cuda_deterministic': True, 'gpu_id': 0, 'log_id': 'dummy', 'check_point_dir': './ck', 'log_dir': './log_dir', 'log_interval': 10, 'save_freq': 250, 'eval_freq': 10000.0, 'env_configs': './configs/pearl_envs.json', 'max_path_length': 200, 'enable_train_eval': False, 'enable_promp_envs': False, 'num_initial_steps': 1000, 'unbounded_eval_hist': True, 'hiddens_conext': [20], 'enable_context': True, 'only_concat_context': 3, 'num_tasks_sample': 1, 'num_train_steps': 500, 'min_buffer_size': 100000, 'history_length': 15, 'beta_clip': 2.0, 'snapshot_size': 2000, 'prox_coef': 0.1, 'meta_batch_size': 10, 'enable_adaptation': True, 'main_snap_iter_nums': 15, 'snap_iter_nums': 5, 'type_of_training': 'td3', 'lam_csc': 0.1, 'use_ess_clipping': False, 'enable_beta_obs_cxt': True, 'sampling_style': 'replay', 'sample_mult': 5, 'use_epi_len_steps': True, 'use_normalized_beta': False, 'reset_optims': False, 'lr_milestone': -1, 'lr_gamma': 0.8}
------------
Read Tasks/Env config params and Update args
eparams:  Namespace(alg_name='mql', batch_size=256, beta_clip=2.0, burn_in=10000, check_point_dir='./ck', cuda_deterministic=True, disable_cuda=False, enable_adaptation=True, enable_beta_obs_cxt=True, enable_context=True, enable_promp_envs=False, enable_train_eval=False, env_configs='./configs/pearl_envs.json', env_name='humanoid-dir', eval_freq=10000.0, expl_noise=0.2, gamma=0.99, gpu_id=0, hidden_sizes=[300, 300], hiddens_conext=[20], history_length=15, lam_csc=0.1, log_dir='./log_dir', log_id='dummy', log_interval=10, lr=0.0003, lr_gamma=0.8, lr_milestone=-1, main_snap_iter_nums=15, max_path_length=200, meta_batch_size=10, min_buffer_size=100000, noise_clip=0.5, num_initial_steps=1000, num_tasks_sample=1, num_train_steps=500, only_concat_context=3, policy_freq=3, policy_noise=0.3, prox_coef=0.1, ptau=0.005, replay_size=500000, reset_optims=False, sample_mult=5, sampling_style='replay', save_freq=250, seed=0, snap_iter_nums=5, snapshot_size=2000, total_timesteps=5000000.0, type_of_training='td3', unbounded_eval_hist=True, use_epi_len_steps=True, use_ess_clipping=False, use_normalized_beta=False)
eparams.env_configs:  ./configs/pearl_envs.json
{'lr': 0.0003, 'replay_size': 500000, 'ptau': 0.005, 'gamma': 0.99, 'burn_in': 10000, 'total_timesteps': 5000000.0, 'expl_noise': 0.2, 'batch_size': 256, 'policy_noise': 0.3, 'noise_clip': 0.5, 'policy_freq': 3, 'hidden_sizes': [300, 300], 'env_name': 'humanoid-dir', 'seed': 0, 'alg_name': 'mql', 'disable_cuda': False, 'cuda_deterministic': True, 'gpu_id': 0, 'log_id': 'dummy', 'check_point_dir': './ck', 'log_dir': './log_dir', 'log_interval': 10, 'save_freq': 250, 'eval_freq': 10000.0, 'env_configs': './configs/pearl_envs.json', 'max_path_length': 200, 'enable_train_eval': False, 'enable_promp_envs': False, 'num_initial_steps': 1000, 'unbounded_eval_hist': True, 'hiddens_conext': [20], 'enable_context': True, 'only_concat_context': 3, 'num_tasks_sample': 1, 'num_train_steps': 500, 'min_buffer_size': 100000, 'history_length': 15, 'beta_clip': 2.0, 'snapshot_size': 2000, 'prox_coef': 0.1, 'meta_batch_size': 10, 'enable_adaptation': True, 'main_snap_iter_nums': 15, 'snap_iter_nums': 5, 'type_of_training': 'td3', 'lam_csc': 0.1, 'use_ess_clipping': False, 'enable_beta_obs_cxt': True, 'sampling_style': 'replay', 'sample_mult': 5, 'use_epi_len_steps': True, 'use_normalized_beta': False, 'reset_optims': False, 'lr_milestone': -1, 'lr_gamma': 0.8, 'n_train_tasks': 100, 'n_eval_tasks': 30, 'n_tasks': 130, 'randomize_tasks': True, 'num_evals': 2, 'num_steps_per_task': 400, 'num_steps_per_eval': 400, 'num_train_steps_per_itr': 4000}
**** No GPU detected or GPU usage is disabled, sorry! ****
Logging to ./log_dir/humanoid-dir_mql_dummy
/home/adrian/studium/master/sem3/rl_project/rlkit/envs/safelife/levels/random/append-dynamic.yaml
wrapped action space:  Box(-0.4, 0.4, (17,), float32)
env:  Normalized: <HumanoidDirEnv instance>
observation_space:  Box(-inf, inf, (376,), float64)
action_space:  Box(-1.0, 1.0, (17,), float32)
observation space shape:  (376,)
action space shape:  (17,)
(17,)
**** TD3 style is selected ****
-----------------------------
Optim Params
Actor:
  Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0003
    maximize: False
    weight_decay: 0
)
Critic:
  Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0003
    maximize: False
    weight_decay: 0
)
********
reset_optims:  False
use_ess_clipping:  False
use_normalized_beta:  False
enable_beta_obs_cxt:  True
********
-----------------------------
runner arguments:  Normalized: <HumanoidDirEnv instance> <algs.MQL.mql.MQL object at 0x7f8aebf770a0> <algs.MQL.buffer.Buffer object at 0x7f8aebf77100> <algs.MQL.multi_tasks_snapshot.MultiTasksSnapshot object at 0x7f8ae97f7280> 10000 0.2 5000000.0 200 15 cpu
train tasks type  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
eval tasks type  [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
-----------------------------
Name of env: humanoid-dir
Observation_space: Box(-inf, inf, (376,), float64)
Action space: Box(-1.0, 1.0, (17,), float32)
Tasks: 130
Train tasks: 100
Eval tasks: 30
######### Using Hist len 15 #########
@@@@@@@@@ Using PEARL Envs @@@@@@@@@
----------------------------
Saving a checkpoint for iteration 0 in ./ck/humanoid-dir_mql_dummy.pt
Eval uses unbounded_eval_hist of length:  200
---------------------------------------
Evaluation over 2 episodes of 30 tasks in episode num 0 and nupdates 0: 124.047351
---------------------------------------
Start burnining for at least 100000
obs shape (376,)
obs size 3136
obs hist size 624
self.replay_buffer size 440
self.tasks_buffer size 29112
new obs size 3136
reward size 48
self.replay_buffer size 55064
self.tasks_buffer size 83736
new obs size 3136
reward size 48
self.replay_buffer size 109640
self.tasks_buffer size 138312
new obs size 3136
reward size 48
self.replay_buffer size 164216
self.tasks_buffer size 192888
new obs size 3136
reward size 48
self.replay_buffer size 218792
self.tasks_buffer size 247464
new obs size 3136
reward size 48
self.replay_buffer size 273400
self.tasks_buffer size 302072
new obs size 3136
reward size 48
self.replay_buffer size 327976
self.tasks_buffer size 356648
new obs size 3136
reward size 48
self.replay_buffer size 382552
self.tasks_buffer size 411224
new obs size 3136
reward size 48
self.replay_buffer size 437128
self.tasks_buffer size 465800
new obs size 3136
reward size 48
self.replay_buffer size 491768
self.tasks_buffer size 520440
new obs size 3136
reward size 48
self.replay_buffer size 546344
self.tasks_buffer size 575016
new obs size 3136
reward size 48
self.replay_buffer size 600920
self.tasks_buffer size 629592
new obs size 3136
reward size 48
self.replay_buffer size 655496
self.tasks_buffer size 684168
new obs size 3136
reward size 48
self.replay_buffer size 710072
self.tasks_buffer size 738744
new obs size 3136
reward size 48
self.replay_buffer size 764648
self.tasks_buffer size 793320
new obs size 3136
reward size 48
self.replay_buffer size 819224
self.tasks_buffer size 847896
new obs size 3136
reward size 48
self.replay_buffer size 873800
self.tasks_buffer size 902472
new obs size 3136
reward size 48
self.replay_buffer size 928448
self.tasks_buffer size 957120
new obs size 3136
reward size 48
self.replay_buffer size 983024
self.tasks_buffer size 1011696
new obs size 3136
reward size 48
self.replay_buffer size 1037600
self.tasks_buffer size 1066272
new obs size 3136
reward size 48
self.replay_buffer size 1092176
self.tasks_buffer size 1120848
new obs size 3136
reward size 48
self.replay_buffer size 1146752
self.tasks_buffer size 1175424
new obs size 3136
reward size 48
Filename: /home/adrian/studium/master/sem3/rl_project/misc/runner_multi_snapshot.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    39    372.0 MiB    372.0 MiB           1       @profile
    40                                             def run(self, update_iter, keep_burning = False, task_id = None, early_leave = 200):
    41                                                 '''
    42                                                     This function add transition to replay buffer.
    43                                                     Early_leave is used in just cold start to collect more data from various tasks,
    44                                                     rather than focus on just few ones
    45                                                 '''
    46    372.0 MiB      0.0 MiB           1           obs = self.env.reset()
    47    372.0 MiB      0.0 MiB           1           print("obs shape", obs.shape)
    48    372.0 MiB      0.0 MiB           1           print("obs size", asizeof.asizeof(obs))
    49    372.0 MiB      0.0 MiB           1           done = False
    50    372.0 MiB      0.0 MiB           1           episode_timesteps = 0
    51    372.0 MiB      0.0 MiB           1           episode_reward = 0
    52    372.0 MiB      0.0 MiB           1           uiter = 0
    53    372.0 MiB      0.0 MiB           1           reward_epinfos = []
    54                                         
    55                                                 ########
    56                                                 ## create a queue to keep track of past rewards and actions
    57                                                 ########
    58    372.0 MiB      0.0 MiB           1           rewards_hist = deque(maxlen=self.hist_len)
    59    372.0 MiB      0.0 MiB           1           actions_hist = deque(maxlen=self.hist_len)
    60    372.0 MiB      0.0 MiB           1           obsvs_hist   = deque(maxlen=self.hist_len)
    61                                         
    62    372.0 MiB      0.0 MiB           1           next_hrews = deque(maxlen=self.hist_len)
    63    372.0 MiB      0.0 MiB           1           next_hacts = deque(maxlen=self.hist_len)
    64    372.0 MiB      0.0 MiB           1           next_hobvs = deque(maxlen=self.hist_len)
    65                                         
    66                                                 # Given batching schema, I need to build a full seq to keep in replay buffer
    67                                                 # Add to all zeros.
    68    372.0 MiB      0.0 MiB           1           zero_action = np.zeros(self.env.action_space.shape[0])
    69    372.0 MiB      0.0 MiB           1           zero_obs    = np.zeros(obs.shape)
    70    372.0 MiB      0.0 MiB          16           for _ in range(self.hist_len):
    71    372.0 MiB      0.0 MiB          15               rewards_hist.append(0)
    72    372.0 MiB      0.0 MiB          15               actions_hist.append(zero_action.copy())
    73    372.0 MiB      0.0 MiB          15               obsvs_hist.append(zero_obs.copy())
    74                                         
    75                                                     # same thing for next_h*
    76    372.0 MiB      0.0 MiB          15               next_hrews.append(0)
    77    372.0 MiB      0.0 MiB          15               next_hacts.append(zero_action.copy())
    78    372.0 MiB      0.0 MiB          15               next_hobvs.append(zero_obs.copy())
    79                                         
    80                                                 # now add obs to the seq
    81    372.0 MiB      0.0 MiB           1           rand_acttion = np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
    82    372.0 MiB      0.0 MiB           1           rand_acttion = rand_acttion.clip(self.env.action_space.low, self.env.action_space.high)
    83    372.0 MiB      0.0 MiB           1           rewards_hist.append(0)
    84    372.0 MiB      0.0 MiB           1           actions_hist.append(rand_acttion.copy())
    85    372.0 MiB      0.0 MiB           1           obsvs_hist.append(obs.copy())
    86                                         
    87    372.0 MiB      0.0 MiB           1           print("obs hist size", asizeof.asizeof(obsvs_hist))
    88                                         
    89                                                 ######
    90                                                 # Start collecting data
    91                                                 #####
    92    372.0 MiB      0.0 MiB          23           while not done and uiter < np.minimum(self.max_path_length, early_leave):
    93                                         
    94    372.0 MiB      0.0 MiB          22               print("self.replay_buffer size", asizeof.asizeof(self.replay_buffer))
    95    372.0 MiB      0.1 MiB          22               print("self.tasks_buffer size", asizeof.asizeof(self.tasks_buffer))
    96                                         
    97                                                     #####
    98                                                     # Convert actions_hist, rewards_hist to np.array and flatten them out
    99                                                     # for example: hist =7, actin_dim = 11 --> np.asarray(actions_hist(7, 11)) ==> flatten ==> (77,)
   100    372.0 MiB      0.0 MiB          22               np_pre_actions = np.asarray(actions_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   101    372.0 MiB      0.0 MiB          22               np_pre_rewards = np.asarray(rewards_hist, dtype=np.float32) #(hist, )
   102    372.0 MiB      0.0 MiB          22               np_pre_obsers = np.asarray(obsvs_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   103                                         
   104                                                     # Select action randomly or according to policy
   105    372.0 MiB      0.0 MiB          22               if keep_burning or update_iter < self.burn_in:
   106    372.0 MiB      0.0 MiB          22                   action = self.env.action_space.sample()
   107                                         
   108                                                     else:
   109                                                         # select_action take into account previous action to take into account
   110                                                         # previous action in selecting a new action
   111                                                         action = self.model.select_action(np.array(obs), np.array(np_pre_actions), np.array(np_pre_rewards), np.array(np_pre_obsers))
   112                                         
   113                                                         if self.expl_noise != 0: 
   114                                                             action = action + np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
   115                                                             action = action.clip(self.env.action_space.low, self.env.action_space.high)
   116                                         
   117                                                     # Perform action
   118    372.0 MiB      0.0 MiB          22               new_obs, reward, done, _ = self.env.step(action) 
   119    372.0 MiB      0.0 MiB          22               print("new obs size", asizeof.asizeof(new_obs))
   120    372.0 MiB      0.0 MiB          22               print("reward size", asizeof.asizeof(reward))
   121                                         
   122    372.0 MiB      0.0 MiB          22               if episode_timesteps + 1 == self.max_path_length:
   123                                                         done_bool = 0
   124                                         
   125                                                     else:
   126    372.0 MiB      0.0 MiB          22                   done_bool = float(done)
   127                                         
   128    372.0 MiB      0.0 MiB          22               episode_reward += reward
   129    372.0 MiB      0.0 MiB          22               reward_epinfos.append(reward)
   130                                         
   131                                                     ###############
   132    372.0 MiB      0.0 MiB          22               next_hrews.append(reward)
   133    372.0 MiB      0.0 MiB          22               next_hacts.append(action.copy())
   134    372.0 MiB      0.0 MiB          22               next_hobvs.append(obs.copy())
   135                                         
   136                                                     # np_next_hacts and np_next_hrews are required for TD3 alg
   137    372.0 MiB      0.0 MiB          22               np_next_hacts = np.asarray(next_hacts, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   138    372.0 MiB      0.0 MiB          22               np_next_hrews = np.asarray(next_hrews, dtype=np.float32) #(hist, )
   139    372.0 MiB      0.0 MiB          22               np_next_hobvs = np.asarray(next_hobvs, dtype=np.float32).flatten() #(hist, )
   140                                         
   141                                                     # Store data in replay buffer
   142    372.0 MiB      0.0 MiB          44               self.replay_buffer.add((obs, new_obs, action, reward, done_bool,
   143    372.0 MiB      0.0 MiB          22                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   144    372.0 MiB      0.0 MiB          22                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   145                                         
   146                                                     # This is snapshot buffer which has short memeory
   147    372.0 MiB      0.0 MiB          44               self.tasks_buffer.add(task_id, (obs, new_obs, action, reward, done_bool,
   148    372.0 MiB      0.0 MiB          22                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   149    372.0 MiB      0.0 MiB          22                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   150                                         
   151                                                     # new becomes old
   152    372.0 MiB      0.0 MiB          22               rewards_hist.append(reward)
   153    372.0 MiB      0.0 MiB          22               actions_hist.append(action.copy())
   154    372.0 MiB      0.0 MiB          22               obsvs_hist.append(obs.copy())
   155                                         
   156    372.0 MiB      0.0 MiB          22               obs = new_obs.copy()
   157    372.0 MiB      0.0 MiB          22               episode_timesteps += 1
   158    372.0 MiB      0.0 MiB          22               update_iter += 1
   159    372.0 MiB      0.0 MiB          22               uiter += 1
   160                                         
   161    372.0 MiB      0.0 MiB           1           info = {}
   162    372.0 MiB      0.0 MiB           1           info['episode_timesteps'] = episode_timesteps
   163    372.0 MiB      0.0 MiB           1           info['update_iter'] = update_iter
   164    372.0 MiB      0.0 MiB           1           info['episode_reward'] = episode_reward
   165    372.0 MiB      0.0 MiB           1           info['epinfos'] = [{"r": round(sum(reward_epinfos), 6), "l": len(reward_epinfos)}]
   166                                         
   167    372.0 MiB      0.0 MiB           1           return info


-----------------
function                      44179    +44179
tuple                         28701    +28701
dict                          28135    +28135
list                          10704    +10704
cell                          10642    +10642
getset_descriptor             10575    +10575
weakref                       10452    +10452
builtin_function_or_method     7612     +7612
type                           6843     +6843
method_descriptor              5822     +5822
env size 54448
rollouts size 1544448
args size 8664
epinfobuf size 624
epinfobuf2 size 624
:::::::::::::::::::::::
leaking objects size:  6292
obs shape (376,)
obs size 3136
obs hist size 624
self.replay_buffer size 1201328
self.tasks_buffer size 1230000
new obs size 3136
reward size 48
self.replay_buffer size 1255904
self.tasks_buffer size 1284608
new obs size 3136
reward size 48
self.replay_buffer size 1310480
self.tasks_buffer size 1339184
new obs size 3136
reward size 48
self.replay_buffer size 1365056
self.tasks_buffer size 1393760
new obs size 3136
reward size 48
self.replay_buffer size 1419712
self.tasks_buffer size 1448336
new obs size 3136
reward size 48
self.replay_buffer size 1474288
self.tasks_buffer size 1502944
new obs size 3136
reward size 48
self.replay_buffer size 1528864
self.tasks_buffer size 1557520
new obs size 3136
reward size 48
self.replay_buffer size 1583440
self.tasks_buffer size 1612096
new obs size 3136
reward size 48
self.replay_buffer size 1638016
self.tasks_buffer size 1666672
new obs size 3136
reward size 48
self.replay_buffer size 1692592
self.tasks_buffer size 1721312
new obs size 3136
reward size 48
self.replay_buffer size 1747168
self.tasks_buffer size 1775888
new obs size 3136
reward size 48
self.replay_buffer size 1801744
self.tasks_buffer size 1830464
new obs size 3136
reward size 48
self.replay_buffer size 1856320
self.tasks_buffer size 1885040
new obs size 3136
reward size 48
self.replay_buffer size 1910896
self.tasks_buffer size 1939616
new obs size 3136
reward size 48
self.replay_buffer size 1965560
self.tasks_buffer size 1994192
new obs size 3136
reward size 48
self.replay_buffer size 2020136
self.tasks_buffer size 2048768
new obs size 3136
reward size 48
self.replay_buffer size 2074712
self.tasks_buffer size 2103344
new obs size 3136
reward size 48
self.replay_buffer size 2129288
self.tasks_buffer size 2157992
new obs size 3136
reward size 48
self.replay_buffer size 2183864
self.tasks_buffer size 2212568
new obs size 3136
reward size 48
self.replay_buffer size 2238440
self.tasks_buffer size 2267144
new obs size 3136
reward size 48
self.replay_buffer size 2293016
self.tasks_buffer size 2321720
new obs size 3136
reward size 48
self.replay_buffer size 2347592
self.tasks_buffer size 2376296
new obs size 3136
reward size 48
self.replay_buffer size 2402168
self.tasks_buffer size 2430872
new obs size 3136
reward size 48
Filename: /home/adrian/studium/master/sem3/rl_project/misc/runner_multi_snapshot.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    39    389.0 MiB    389.0 MiB           1       @profile
    40                                             def run(self, update_iter, keep_burning = False, task_id = None, early_leave = 200):
    41                                                 '''
    42                                                     This function add transition to replay buffer.
    43                                                     Early_leave is used in just cold start to collect more data from various tasks,
    44                                                     rather than focus on just few ones
    45                                                 '''
    46    389.0 MiB      0.0 MiB           1           obs = self.env.reset()
    47    389.0 MiB      0.0 MiB           1           print("obs shape", obs.shape)
    48    389.0 MiB      0.0 MiB           1           print("obs size", asizeof.asizeof(obs))
    49    389.0 MiB      0.0 MiB           1           done = False
    50    389.0 MiB      0.0 MiB           1           episode_timesteps = 0
    51    389.0 MiB      0.0 MiB           1           episode_reward = 0
    52    389.0 MiB      0.0 MiB           1           uiter = 0
    53    389.0 MiB      0.0 MiB           1           reward_epinfos = []
    54                                         
    55                                                 ########
    56                                                 ## create a queue to keep track of past rewards and actions
    57                                                 ########
    58    389.0 MiB      0.0 MiB           1           rewards_hist = deque(maxlen=self.hist_len)
    59    389.0 MiB      0.0 MiB           1           actions_hist = deque(maxlen=self.hist_len)
    60    389.0 MiB      0.0 MiB           1           obsvs_hist   = deque(maxlen=self.hist_len)
    61                                         
    62    389.0 MiB      0.0 MiB           1           next_hrews = deque(maxlen=self.hist_len)
    63    389.0 MiB      0.0 MiB           1           next_hacts = deque(maxlen=self.hist_len)
    64    389.0 MiB      0.0 MiB           1           next_hobvs = deque(maxlen=self.hist_len)
    65                                         
    66                                                 # Given batching schema, I need to build a full seq to keep in replay buffer
    67                                                 # Add to all zeros.
    68    389.0 MiB      0.0 MiB           1           zero_action = np.zeros(self.env.action_space.shape[0])
    69    389.0 MiB      0.0 MiB           1           zero_obs    = np.zeros(obs.shape)
    70    389.0 MiB      0.0 MiB          16           for _ in range(self.hist_len):
    71    389.0 MiB      0.0 MiB          15               rewards_hist.append(0)
    72    389.0 MiB      0.0 MiB          15               actions_hist.append(zero_action.copy())
    73    389.0 MiB      0.0 MiB          15               obsvs_hist.append(zero_obs.copy())
    74                                         
    75                                                     # same thing for next_h*
    76    389.0 MiB      0.0 MiB          15               next_hrews.append(0)
    77    389.0 MiB      0.0 MiB          15               next_hacts.append(zero_action.copy())
    78    389.0 MiB      0.0 MiB          15               next_hobvs.append(zero_obs.copy())
    79                                         
    80                                                 # now add obs to the seq
    81    389.0 MiB      0.0 MiB           1           rand_acttion = np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
    82    389.0 MiB      0.0 MiB           1           rand_acttion = rand_acttion.clip(self.env.action_space.low, self.env.action_space.high)
    83    389.0 MiB      0.0 MiB           1           rewards_hist.append(0)
    84    389.0 MiB      0.0 MiB           1           actions_hist.append(rand_acttion.copy())
    85    389.0 MiB      0.0 MiB           1           obsvs_hist.append(obs.copy())
    86                                         
    87    389.0 MiB      0.0 MiB           1           print("obs hist size", asizeof.asizeof(obsvs_hist))
    88                                         
    89                                                 ######
    90                                                 # Start collecting data
    91                                                 #####
    92    389.0 MiB      0.0 MiB          24           while not done and uiter < np.minimum(self.max_path_length, early_leave):
    93                                         
    94    389.0 MiB      0.0 MiB          23               print("self.replay_buffer size", asizeof.asizeof(self.replay_buffer))
    95    389.0 MiB      0.0 MiB          23               print("self.tasks_buffer size", asizeof.asizeof(self.tasks_buffer))
    96                                         
    97                                                     #####
    98                                                     # Convert actions_hist, rewards_hist to np.array and flatten them out
    99                                                     # for example: hist =7, actin_dim = 11 --> np.asarray(actions_hist(7, 11)) ==> flatten ==> (77,)
   100    389.0 MiB      0.0 MiB          23               np_pre_actions = np.asarray(actions_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   101    389.0 MiB      0.0 MiB          23               np_pre_rewards = np.asarray(rewards_hist, dtype=np.float32) #(hist, )
   102    389.0 MiB      0.0 MiB          23               np_pre_obsers = np.asarray(obsvs_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   103                                         
   104                                                     # Select action randomly or according to policy
   105    389.0 MiB      0.0 MiB          23               if keep_burning or update_iter < self.burn_in:
   106    389.0 MiB      0.0 MiB          23                   action = self.env.action_space.sample()
   107                                         
   108                                                     else:
   109                                                         # select_action take into account previous action to take into account
   110                                                         # previous action in selecting a new action
   111                                                         action = self.model.select_action(np.array(obs), np.array(np_pre_actions), np.array(np_pre_rewards), np.array(np_pre_obsers))
   112                                         
   113                                                         if self.expl_noise != 0: 
   114                                                             action = action + np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
   115                                                             action = action.clip(self.env.action_space.low, self.env.action_space.high)
   116                                         
   117                                                     # Perform action
   118    389.0 MiB      0.0 MiB          23               new_obs, reward, done, _ = self.env.step(action) 
   119    389.0 MiB      0.0 MiB          23               print("new obs size", asizeof.asizeof(new_obs))
   120    389.0 MiB      0.0 MiB          23               print("reward size", asizeof.asizeof(reward))
   121                                         
   122    389.0 MiB      0.0 MiB          23               if episode_timesteps + 1 == self.max_path_length:
   123                                                         done_bool = 0
   124                                         
   125                                                     else:
   126    389.0 MiB      0.0 MiB          23                   done_bool = float(done)
   127                                         
   128    389.0 MiB      0.0 MiB          23               episode_reward += reward
   129    389.0 MiB      0.0 MiB          23               reward_epinfos.append(reward)
   130                                         
   131                                                     ###############
   132    389.0 MiB      0.0 MiB          23               next_hrews.append(reward)
   133    389.0 MiB      0.0 MiB          23               next_hacts.append(action.copy())
   134    389.0 MiB      0.0 MiB          23               next_hobvs.append(obs.copy())
   135                                         
   136                                                     # np_next_hacts and np_next_hrews are required for TD3 alg
   137    389.0 MiB      0.0 MiB          23               np_next_hacts = np.asarray(next_hacts, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   138    389.0 MiB      0.0 MiB          23               np_next_hrews = np.asarray(next_hrews, dtype=np.float32) #(hist, )
   139    389.0 MiB      0.0 MiB          23               np_next_hobvs = np.asarray(next_hobvs, dtype=np.float32).flatten() #(hist, )
   140                                         
   141                                                     # Store data in replay buffer
   142    389.0 MiB      0.0 MiB          46               self.replay_buffer.add((obs, new_obs, action, reward, done_bool,
   143    389.0 MiB      0.0 MiB          23                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   144    389.0 MiB      0.0 MiB          23                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   145                                         
   146                                                     # This is snapshot buffer which has short memeory
   147    389.0 MiB      0.0 MiB          46               self.tasks_buffer.add(task_id, (obs, new_obs, action, reward, done_bool,
   148    389.0 MiB      0.0 MiB          23                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   149    389.0 MiB      0.0 MiB          23                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   150                                         
   151                                                     # new becomes old
   152    389.0 MiB      0.0 MiB          23               rewards_hist.append(reward)
   153    389.0 MiB      0.0 MiB          23               actions_hist.append(action.copy())
   154    389.0 MiB      0.0 MiB          23               obsvs_hist.append(obs.copy())
   155                                         
   156    389.0 MiB      0.0 MiB          23               obs = new_obs.copy()
   157    389.0 MiB      0.0 MiB          23               episode_timesteps += 1
   158    389.0 MiB      0.0 MiB          23               update_iter += 1
   159    389.0 MiB      0.0 MiB          23               uiter += 1
   160                                         
   161    389.0 MiB      0.0 MiB           1           info = {}
   162    389.0 MiB      0.0 MiB           1           info['episode_timesteps'] = episode_timesteps
   163    389.0 MiB      0.0 MiB           1           info['update_iter'] = update_iter
   164    389.0 MiB      0.0 MiB           1           info['episode_reward'] = episode_reward
   165    389.0 MiB      0.0 MiB           1           info['epinfos'] = [{"r": round(sum(reward_epinfos), 6), "l": len(reward_epinfos)}]
   166                                         
   167    389.0 MiB      0.0 MiB           1           return info


-----------------
_Typedef      158       +47
_Claskey       74       +19
env size 54448
rollouts size 2803008
args size 8664
epinfobuf size 624
epinfobuf2 size 624
:::::::::::::::::::::::
leaking objects size:  6292
obs shape (376,)
obs size 3136
obs hist size 624
self.replay_buffer size 2456744
self.tasks_buffer size 2485448
new obs size 3136
reward size 48
self.replay_buffer size 2511320
self.tasks_buffer size 2540056
new obs size 3136
reward size 48
self.replay_buffer size 2565992
self.tasks_buffer size 2594632
new obs size 3136
reward size 48
self.replay_buffer size 2620568
self.tasks_buffer size 2649208
new obs size 3136
reward size 48
self.replay_buffer size 2675144
self.tasks_buffer size 2703784
new obs size 3136
reward size 48
self.replay_buffer size 2729720
self.tasks_buffer size 2758392
new obs size 3136
reward size 48
self.replay_buffer size 2784296
self.tasks_buffer size 2812968
new obs size 3136
reward size 48
self.replay_buffer size 2838872
self.tasks_buffer size 2867544
new obs size 3136
reward size 48
self.replay_buffer size 2893448
self.tasks_buffer size 2922120
new obs size 3136
reward size 48
self.replay_buffer size 2948024
self.tasks_buffer size 2976760
new obs size 3136
reward size 48
self.replay_buffer size 3002600
self.tasks_buffer size 3031336
new obs size 3136
reward size 48
self.replay_buffer size 3057176
self.tasks_buffer size 3085912
new obs size 3136
reward size 48
self.replay_buffer size 3111752
self.tasks_buffer size 3140488
new obs size 3136
reward size 48
self.replay_buffer size 3166328
self.tasks_buffer size 3195064
new obs size 3136
reward size 48
self.replay_buffer size 3221016
self.tasks_buffer size 3249640
new obs size 3136
reward size 48
self.replay_buffer size 3275592
self.tasks_buffer size 3304216
new obs size 3136
reward size 48
self.replay_buffer size 3330168
self.tasks_buffer size 3358792
new obs size 3136
reward size 48
Filename: /home/adrian/studium/master/sem3/rl_project/misc/runner_multi_snapshot.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    39    390.2 MiB    390.2 MiB           1       @profile
    40                                             def run(self, update_iter, keep_burning = False, task_id = None, early_leave = 200):
    41                                                 '''
    42                                                     This function add transition to replay buffer.
    43                                                     Early_leave is used in just cold start to collect more data from various tasks,
    44                                                     rather than focus on just few ones
    45                                                 '''
    46    390.2 MiB      0.0 MiB           1           obs = self.env.reset()
    47    390.2 MiB      0.0 MiB           1           print("obs shape", obs.shape)
    48    390.2 MiB      0.0 MiB           1           print("obs size", asizeof.asizeof(obs))
    49    390.2 MiB      0.0 MiB           1           done = False
    50    390.2 MiB      0.0 MiB           1           episode_timesteps = 0
    51    390.2 MiB      0.0 MiB           1           episode_reward = 0
    52    390.2 MiB      0.0 MiB           1           uiter = 0
    53    390.2 MiB      0.0 MiB           1           reward_epinfos = []
    54                                         
    55                                                 ########
    56                                                 ## create a queue to keep track of past rewards and actions
    57                                                 ########
    58    390.2 MiB      0.0 MiB           1           rewards_hist = deque(maxlen=self.hist_len)
    59    390.2 MiB      0.0 MiB           1           actions_hist = deque(maxlen=self.hist_len)
    60    390.2 MiB      0.0 MiB           1           obsvs_hist   = deque(maxlen=self.hist_len)
    61                                         
    62    390.2 MiB      0.0 MiB           1           next_hrews = deque(maxlen=self.hist_len)
    63    390.2 MiB      0.0 MiB           1           next_hacts = deque(maxlen=self.hist_len)
    64    390.2 MiB      0.0 MiB           1           next_hobvs = deque(maxlen=self.hist_len)
    65                                         
    66                                                 # Given batching schema, I need to build a full seq to keep in replay buffer
    67                                                 # Add to all zeros.
    68    390.2 MiB      0.0 MiB           1           zero_action = np.zeros(self.env.action_space.shape[0])
    69    390.2 MiB      0.0 MiB           1           zero_obs    = np.zeros(obs.shape)
    70    390.2 MiB      0.0 MiB          16           for _ in range(self.hist_len):
    71    390.2 MiB      0.0 MiB          15               rewards_hist.append(0)
    72    390.2 MiB      0.0 MiB          15               actions_hist.append(zero_action.copy())
    73    390.2 MiB      0.0 MiB          15               obsvs_hist.append(zero_obs.copy())
    74                                         
    75                                                     # same thing for next_h*
    76    390.2 MiB      0.0 MiB          15               next_hrews.append(0)
    77    390.2 MiB      0.0 MiB          15               next_hacts.append(zero_action.copy())
    78    390.2 MiB      0.0 MiB          15               next_hobvs.append(zero_obs.copy())
    79                                         
    80                                                 # now add obs to the seq
    81    390.2 MiB      0.0 MiB           1           rand_acttion = np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
    82    390.2 MiB      0.0 MiB           1           rand_acttion = rand_acttion.clip(self.env.action_space.low, self.env.action_space.high)
    83    390.2 MiB      0.0 MiB           1           rewards_hist.append(0)
    84    390.2 MiB      0.0 MiB           1           actions_hist.append(rand_acttion.copy())
    85    390.2 MiB      0.0 MiB           1           obsvs_hist.append(obs.copy())
    86                                         
    87    390.2 MiB      0.0 MiB           1           print("obs hist size", asizeof.asizeof(obsvs_hist))
    88                                         
    89                                                 ######
    90                                                 # Start collecting data
    91                                                 #####
    92    390.2 MiB      0.0 MiB          18           while not done and uiter < np.minimum(self.max_path_length, early_leave):
    93                                         
    94    390.2 MiB      0.0 MiB          17               print("self.replay_buffer size", asizeof.asizeof(self.replay_buffer))
    95    390.2 MiB      0.0 MiB          17               print("self.tasks_buffer size", asizeof.asizeof(self.tasks_buffer))
    96                                         
    97                                                     #####
    98                                                     # Convert actions_hist, rewards_hist to np.array and flatten them out
    99                                                     # for example: hist =7, actin_dim = 11 --> np.asarray(actions_hist(7, 11)) ==> flatten ==> (77,)
   100    390.2 MiB      0.0 MiB          17               np_pre_actions = np.asarray(actions_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   101    390.2 MiB      0.0 MiB          17               np_pre_rewards = np.asarray(rewards_hist, dtype=np.float32) #(hist, )
   102    390.2 MiB      0.0 MiB          17               np_pre_obsers = np.asarray(obsvs_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   103                                         
   104                                                     # Select action randomly or according to policy
   105    390.2 MiB      0.0 MiB          17               if keep_burning or update_iter < self.burn_in:
   106    390.2 MiB      0.0 MiB          17                   action = self.env.action_space.sample()
   107                                         
   108                                                     else:
   109                                                         # select_action take into account previous action to take into account
   110                                                         # previous action in selecting a new action
   111                                                         action = self.model.select_action(np.array(obs), np.array(np_pre_actions), np.array(np_pre_rewards), np.array(np_pre_obsers))
   112                                         
   113                                                         if self.expl_noise != 0: 
   114                                                             action = action + np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
   115                                                             action = action.clip(self.env.action_space.low, self.env.action_space.high)
   116                                         
   117                                                     # Perform action
   118    390.2 MiB      0.0 MiB          17               new_obs, reward, done, _ = self.env.step(action) 
   119    390.2 MiB      0.0 MiB          17               print("new obs size", asizeof.asizeof(new_obs))
   120    390.2 MiB      0.0 MiB          17               print("reward size", asizeof.asizeof(reward))
   121                                         
   122    390.2 MiB      0.0 MiB          17               if episode_timesteps + 1 == self.max_path_length:
   123                                                         done_bool = 0
   124                                         
   125                                                     else:
   126    390.2 MiB      0.0 MiB          17                   done_bool = float(done)
   127                                         
   128    390.2 MiB      0.0 MiB          17               episode_reward += reward
   129    390.2 MiB      0.0 MiB          17               reward_epinfos.append(reward)
   130                                         
   131                                                     ###############
   132    390.2 MiB      0.0 MiB          17               next_hrews.append(reward)
   133    390.2 MiB      0.0 MiB          17               next_hacts.append(action.copy())
   134    390.2 MiB      0.0 MiB          17               next_hobvs.append(obs.copy())
   135                                         
   136                                                     # np_next_hacts and np_next_hrews are required for TD3 alg
   137    390.2 MiB      0.0 MiB          17               np_next_hacts = np.asarray(next_hacts, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   138    390.2 MiB      0.0 MiB          17               np_next_hrews = np.asarray(next_hrews, dtype=np.float32) #(hist, )
   139    390.2 MiB      0.0 MiB          17               np_next_hobvs = np.asarray(next_hobvs, dtype=np.float32).flatten() #(hist, )
   140                                         
   141                                                     # Store data in replay buffer
   142    390.2 MiB      0.0 MiB          34               self.replay_buffer.add((obs, new_obs, action, reward, done_bool,
   143    390.2 MiB      0.0 MiB          17                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   144    390.2 MiB      0.0 MiB          17                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   145                                         
   146                                                     # This is snapshot buffer which has short memeory
   147    390.2 MiB      0.0 MiB          34               self.tasks_buffer.add(task_id, (obs, new_obs, action, reward, done_bool,
   148    390.2 MiB      0.0 MiB          17                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   149    390.2 MiB      0.0 MiB          17                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   150                                         
   151                                                     # new becomes old
   152    390.2 MiB      0.0 MiB          17               rewards_hist.append(reward)
   153    390.2 MiB      0.0 MiB          17               actions_hist.append(action.copy())
   154    390.2 MiB      0.0 MiB          17               obsvs_hist.append(obs.copy())
   155                                         
   156    390.2 MiB      0.0 MiB          17               obs = new_obs.copy()
   157    390.2 MiB      0.0 MiB          17               episode_timesteps += 1
   158    390.2 MiB      0.0 MiB          17               update_iter += 1
   159    390.2 MiB      0.0 MiB          17               uiter += 1
   160                                         
   161    390.2 MiB      0.0 MiB           1           info = {}
   162    390.2 MiB      0.0 MiB           1           info['episode_timesteps'] = episode_timesteps
   163    390.2 MiB      0.0 MiB           1           info['update_iter'] = update_iter
   164    390.2 MiB      0.0 MiB           1           info['episode_reward'] = episode_reward
   165    390.2 MiB      0.0 MiB           1           info['epinfos'] = [{"r": round(sum(reward_epinfos), 6), "l": len(reward_epinfos)}]
   166                                         
   167    390.2 MiB      0.0 MiB           1           return info


-----------------
env size 54448
rollouts size 3733384
args size 8664
epinfobuf size 624
epinfobuf2 size 624
:::::::::::::::::::::::
leaking objects size:  6292
obs shape (376,)
obs size 3136
obs hist size 624
self.replay_buffer size 3384744
self.tasks_buffer size 3413440
new obs size 3136
reward size 48
self.replay_buffer size 3439320
self.tasks_buffer size 3468048
new obs size 3136
reward size 48
self.replay_buffer size 3493896
self.tasks_buffer size 3522624
new obs size 3136
reward size 48
self.replay_buffer size 3548472
self.tasks_buffer size 3577200
new obs size 3136
reward size 48
self.replay_buffer size 3603048
self.tasks_buffer size 3631776
new obs size 3136
reward size 48
self.replay_buffer size 3657624
self.tasks_buffer size 3686384
new obs size 3136
reward size 48
self.replay_buffer size 3712200
self.tasks_buffer size 3740960
new obs size 3136
reward size 48
self.replay_buffer size 3766776
self.tasks_buffer size 3795536
new obs size 3136
reward size 48
self.replay_buffer size 3821352
self.tasks_buffer size 3850112
new obs size 3136
reward size 48
self.replay_buffer size 3875928
self.tasks_buffer size 3904752
new obs size 3136
reward size 48
self.replay_buffer size 3930504
self.tasks_buffer size 3959328
new obs size 3136
reward size 48
self.replay_buffer size 3985208
self.tasks_buffer size 4013904
new obs size 3136
reward size 48
self.replay_buffer size 4039784
self.tasks_buffer size 4068480
new obs size 3136
reward size 48
self.replay_buffer size 4094360
self.tasks_buffer size 4123056
new obs size 3136
reward size 48
self.replay_buffer size 4148936
self.tasks_buffer size 4177632
new obs size 3136
reward size 48
self.replay_buffer size 4203512
self.tasks_buffer size 4232208
new obs size 3136
reward size 48
self.replay_buffer size 4258088
self.tasks_buffer size 4286784
new obs size 3136
reward size 48
self.replay_buffer size 4312664
self.tasks_buffer size 4341432
new obs size 3136
reward size 48
Filename: /home/adrian/studium/master/sem3/rl_project/misc/runner_multi_snapshot.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    39    391.0 MiB    391.0 MiB           1       @profile
    40                                             def run(self, update_iter, keep_burning = False, task_id = None, early_leave = 200):
    41                                                 '''
    42                                                     This function add transition to replay buffer.
    43                                                     Early_leave is used in just cold start to collect more data from various tasks,
    44                                                     rather than focus on just few ones
    45                                                 '''
    46    391.0 MiB      0.0 MiB           1           obs = self.env.reset()
    47    391.0 MiB      0.0 MiB           1           print("obs shape", obs.shape)
    48    391.0 MiB      0.0 MiB           1           print("obs size", asizeof.asizeof(obs))
    49    391.0 MiB      0.0 MiB           1           done = False
    50    391.0 MiB      0.0 MiB           1           episode_timesteps = 0
    51    391.0 MiB      0.0 MiB           1           episode_reward = 0
    52    391.0 MiB      0.0 MiB           1           uiter = 0
    53    391.0 MiB      0.0 MiB           1           reward_epinfos = []
    54                                         
    55                                                 ########
    56                                                 ## create a queue to keep track of past rewards and actions
    57                                                 ########
    58    391.0 MiB      0.0 MiB           1           rewards_hist = deque(maxlen=self.hist_len)
    59    391.0 MiB      0.0 MiB           1           actions_hist = deque(maxlen=self.hist_len)
    60    391.0 MiB      0.0 MiB           1           obsvs_hist   = deque(maxlen=self.hist_len)
    61                                         
    62    391.0 MiB      0.0 MiB           1           next_hrews = deque(maxlen=self.hist_len)
    63    391.0 MiB      0.0 MiB           1           next_hacts = deque(maxlen=self.hist_len)
    64    391.0 MiB      0.0 MiB           1           next_hobvs = deque(maxlen=self.hist_len)
    65                                         
    66                                                 # Given batching schema, I need to build a full seq to keep in replay buffer
    67                                                 # Add to all zeros.
    68    391.0 MiB      0.0 MiB           1           zero_action = np.zeros(self.env.action_space.shape[0])
    69    391.0 MiB      0.0 MiB           1           zero_obs    = np.zeros(obs.shape)
    70    391.0 MiB      0.0 MiB          16           for _ in range(self.hist_len):
    71    391.0 MiB      0.0 MiB          15               rewards_hist.append(0)
    72    391.0 MiB      0.0 MiB          15               actions_hist.append(zero_action.copy())
    73    391.0 MiB      0.0 MiB          15               obsvs_hist.append(zero_obs.copy())
    74                                         
    75                                                     # same thing for next_h*
    76    391.0 MiB      0.0 MiB          15               next_hrews.append(0)
    77    391.0 MiB      0.0 MiB          15               next_hacts.append(zero_action.copy())
    78    391.0 MiB      0.0 MiB          15               next_hobvs.append(zero_obs.copy())
    79                                         
    80                                                 # now add obs to the seq
    81    391.0 MiB      0.0 MiB           1           rand_acttion = np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
    82    391.0 MiB      0.0 MiB           1           rand_acttion = rand_acttion.clip(self.env.action_space.low, self.env.action_space.high)
    83    391.0 MiB      0.0 MiB           1           rewards_hist.append(0)
    84    391.0 MiB      0.0 MiB           1           actions_hist.append(rand_acttion.copy())
    85    391.0 MiB      0.0 MiB           1           obsvs_hist.append(obs.copy())
    86                                         
    87    391.0 MiB      0.0 MiB           1           print("obs hist size", asizeof.asizeof(obsvs_hist))
    88                                         
    89                                                 ######
    90                                                 # Start collecting data
    91                                                 #####
    92    391.0 MiB      0.0 MiB          19           while not done and uiter < np.minimum(self.max_path_length, early_leave):
    93                                         
    94    391.0 MiB      0.0 MiB          18               print("self.replay_buffer size", asizeof.asizeof(self.replay_buffer))
    95    391.0 MiB      0.0 MiB          18               print("self.tasks_buffer size", asizeof.asizeof(self.tasks_buffer))
    96                                         
    97                                                     #####
    98                                                     # Convert actions_hist, rewards_hist to np.array and flatten them out
    99                                                     # for example: hist =7, actin_dim = 11 --> np.asarray(actions_hist(7, 11)) ==> flatten ==> (77,)
   100    391.0 MiB      0.0 MiB          18               np_pre_actions = np.asarray(actions_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   101    391.0 MiB      0.0 MiB          18               np_pre_rewards = np.asarray(rewards_hist, dtype=np.float32) #(hist, )
   102    391.0 MiB      0.0 MiB          18               np_pre_obsers = np.asarray(obsvs_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   103                                         
   104                                                     # Select action randomly or according to policy
   105    391.0 MiB      0.0 MiB          18               if keep_burning or update_iter < self.burn_in:
   106    391.0 MiB      0.0 MiB          18                   action = self.env.action_space.sample()
   107                                         
   108                                                     else:
   109                                                         # select_action take into account previous action to take into account
   110                                                         # previous action in selecting a new action
   111                                                         action = self.model.select_action(np.array(obs), np.array(np_pre_actions), np.array(np_pre_rewards), np.array(np_pre_obsers))
   112                                         
   113                                                         if self.expl_noise != 0: 
   114                                                             action = action + np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
   115                                                             action = action.clip(self.env.action_space.low, self.env.action_space.high)
   116                                         
   117                                                     # Perform action
   118    391.0 MiB      0.0 MiB          18               new_obs, reward, done, _ = self.env.step(action) 
   119    391.0 MiB      0.0 MiB          18               print("new obs size", asizeof.asizeof(new_obs))
   120    391.0 MiB      0.0 MiB          18               print("reward size", asizeof.asizeof(reward))
   121                                         
   122    391.0 MiB      0.0 MiB          18               if episode_timesteps + 1 == self.max_path_length:
   123                                                         done_bool = 0
   124                                         
   125                                                     else:
   126    391.0 MiB      0.0 MiB          18                   done_bool = float(done)
   127                                         
   128    391.0 MiB      0.0 MiB          18               episode_reward += reward
   129    391.0 MiB      0.0 MiB          18               reward_epinfos.append(reward)
   130                                         
   131                                                     ###############
   132    391.0 MiB      0.0 MiB          18               next_hrews.append(reward)
   133    391.0 MiB      0.0 MiB          18               next_hacts.append(action.copy())
   134    391.0 MiB      0.0 MiB          18               next_hobvs.append(obs.copy())
   135                                         
   136                                                     # np_next_hacts and np_next_hrews are required for TD3 alg
   137    391.0 MiB      0.0 MiB          18               np_next_hacts = np.asarray(next_hacts, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   138    391.0 MiB      0.0 MiB          18               np_next_hrews = np.asarray(next_hrews, dtype=np.float32) #(hist, )
   139    391.0 MiB      0.0 MiB          18               np_next_hobvs = np.asarray(next_hobvs, dtype=np.float32).flatten() #(hist, )
   140                                         
   141                                                     # Store data in replay buffer
   142    391.0 MiB      0.0 MiB          36               self.replay_buffer.add((obs, new_obs, action, reward, done_bool,
   143    391.0 MiB      0.0 MiB          18                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   144    391.0 MiB      0.0 MiB          18                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   145                                         
   146                                                     # This is snapshot buffer which has short memeory
   147    391.0 MiB      0.0 MiB          36               self.tasks_buffer.add(task_id, (obs, new_obs, action, reward, done_bool,
   148    391.0 MiB      0.0 MiB          18                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   149    391.0 MiB      0.0 MiB          18                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   150                                         
   151                                                     # new becomes old
   152    391.0 MiB      0.0 MiB          18               rewards_hist.append(reward)
   153    391.0 MiB      0.0 MiB          18               actions_hist.append(action.copy())
   154    391.0 MiB      0.0 MiB          18               obsvs_hist.append(obs.copy())
   155                                         
   156    391.0 MiB      0.0 MiB          18               obs = new_obs.copy()
   157    391.0 MiB      0.0 MiB          18               episode_timesteps += 1
   158    391.0 MiB      0.0 MiB          18               update_iter += 1
   159    391.0 MiB      0.0 MiB          18               uiter += 1
   160                                         
   161    391.0 MiB      0.0 MiB           1           info = {}
   162    391.0 MiB      0.0 MiB           1           info['episode_timesteps'] = episode_timesteps
   163    391.0 MiB      0.0 MiB           1           info['update_iter'] = update_iter
   164    391.0 MiB      0.0 MiB           1           info['episode_reward'] = episode_reward
   165    391.0 MiB      0.0 MiB           1           info['epinfos'] = [{"r": round(sum(reward_epinfos), 6), "l": len(reward_epinfos)}]
   166                                         
   167    391.0 MiB      0.0 MiB           1           return info


-----------------
env size 54448
rollouts size 4718384
args size 8664
epinfobuf size 624
epinfobuf2 size 624
:::::::::::::::::::::::
leaking objects size:  6292
obs shape (376,)
obs size 3136
obs hist size 624
self.replay_buffer size 4367240
self.tasks_buffer size 4396008
new obs size 3136
reward size 48
self.replay_buffer size 4421816
self.tasks_buffer size 4450616
new obs size 3136
reward size 48
self.replay_buffer size 4476392
self.tasks_buffer size 4505192
new obs size 3136
reward size 48
self.replay_buffer size 4530968
self.tasks_buffer size 4559768
new obs size 3136
reward size 48
self.replay_buffer size 4585544
self.tasks_buffer size 4614344
new obs size 3136
reward size 48
self.replay_buffer size 4640120
self.tasks_buffer size 4668952
new obs size 3136
reward size 48
self.replay_buffer size 4694696
self.tasks_buffer size 4723528
new obs size 3136
reward size 48
self.replay_buffer size 4749272
self.tasks_buffer size 4778104
new obs size 3136
reward size 48
self.replay_buffer size 4803848
self.tasks_buffer size 4832680
new obs size 3136
reward size 48
self.replay_buffer size 4858568
self.tasks_buffer size 4887320
new obs size 3136
reward size 48
self.replay_buffer size 4913144
self.tasks_buffer size 4941896
new obs size 3136
reward size 48
self.replay_buffer size 4967720
self.tasks_buffer size 4996472
new obs size 3136
reward size 48
self.replay_buffer size 5022296
self.tasks_buffer size 5051048
new obs size 3136
reward size 48
self.replay_buffer size 5076872
self.tasks_buffer size 5105624
new obs size 3136
reward size 48
self.replay_buffer size 5131448
self.tasks_buffer size 5160200
new obs size 3136
reward size 48
self.replay_buffer size 5186024
self.tasks_buffer size 5214776
new obs size 3136
reward size 48
self.replay_buffer size 5240600
self.tasks_buffer size 5269352
new obs size 3136
reward size 48
self.replay_buffer size 5295176
self.tasks_buffer size 5324000
new obs size 3136
reward size 48
self.replay_buffer size 5349752
self.tasks_buffer size 5378576
new obs size 3136
reward size 48
Filename: /home/adrian/studium/master/sem3/rl_project/misc/runner_multi_snapshot.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    39    392.0 MiB    392.0 MiB           1       @profile
    40                                             def run(self, update_iter, keep_burning = False, task_id = None, early_leave = 200):
    41                                                 '''
    42                                                     This function add transition to replay buffer.
    43                                                     Early_leave is used in just cold start to collect more data from various tasks,
    44                                                     rather than focus on just few ones
    45                                                 '''
    46    392.0 MiB      0.0 MiB           1           obs = self.env.reset()
    47    392.0 MiB      0.0 MiB           1           print("obs shape", obs.shape)
    48    392.0 MiB      0.0 MiB           1           print("obs size", asizeof.asizeof(obs))
    49    392.0 MiB      0.0 MiB           1           done = False
    50    392.0 MiB      0.0 MiB           1           episode_timesteps = 0
    51    392.0 MiB      0.0 MiB           1           episode_reward = 0
    52    392.0 MiB      0.0 MiB           1           uiter = 0
    53    392.0 MiB      0.0 MiB           1           reward_epinfos = []
    54                                         
    55                                                 ########
    56                                                 ## create a queue to keep track of past rewards and actions
    57                                                 ########
    58    392.0 MiB      0.0 MiB           1           rewards_hist = deque(maxlen=self.hist_len)
    59    392.0 MiB      0.0 MiB           1           actions_hist = deque(maxlen=self.hist_len)
    60    392.0 MiB      0.0 MiB           1           obsvs_hist   = deque(maxlen=self.hist_len)
    61                                         
    62    392.0 MiB      0.0 MiB           1           next_hrews = deque(maxlen=self.hist_len)
    63    392.0 MiB      0.0 MiB           1           next_hacts = deque(maxlen=self.hist_len)
    64    392.0 MiB      0.0 MiB           1           next_hobvs = deque(maxlen=self.hist_len)
    65                                         
    66                                                 # Given batching schema, I need to build a full seq to keep in replay buffer
    67                                                 # Add to all zeros.
    68    392.0 MiB      0.0 MiB           1           zero_action = np.zeros(self.env.action_space.shape[0])
    69    392.0 MiB      0.0 MiB           1           zero_obs    = np.zeros(obs.shape)
    70    392.0 MiB      0.0 MiB          16           for _ in range(self.hist_len):
    71    392.0 MiB      0.0 MiB          15               rewards_hist.append(0)
    72    392.0 MiB      0.0 MiB          15               actions_hist.append(zero_action.copy())
    73    392.0 MiB      0.0 MiB          15               obsvs_hist.append(zero_obs.copy())
    74                                         
    75                                                     # same thing for next_h*
    76    392.0 MiB      0.0 MiB          15               next_hrews.append(0)
    77    392.0 MiB      0.0 MiB          15               next_hacts.append(zero_action.copy())
    78    392.0 MiB      0.0 MiB          15               next_hobvs.append(zero_obs.copy())
    79                                         
    80                                                 # now add obs to the seq
    81    392.0 MiB      0.0 MiB           1           rand_acttion = np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
    82    392.0 MiB      0.0 MiB           1           rand_acttion = rand_acttion.clip(self.env.action_space.low, self.env.action_space.high)
    83    392.0 MiB      0.0 MiB           1           rewards_hist.append(0)
    84    392.0 MiB      0.0 MiB           1           actions_hist.append(rand_acttion.copy())
    85    392.0 MiB      0.0 MiB           1           obsvs_hist.append(obs.copy())
    86                                         
    87    392.0 MiB      0.0 MiB           1           print("obs hist size", asizeof.asizeof(obsvs_hist))
    88                                         
    89                                                 ######
    90                                                 # Start collecting data
    91                                                 #####
    92    392.0 MiB      0.0 MiB          20           while not done and uiter < np.minimum(self.max_path_length, early_leave):
    93                                         
    94    392.0 MiB      0.0 MiB          19               print("self.replay_buffer size", asizeof.asizeof(self.replay_buffer))
    95    392.0 MiB      0.0 MiB          19               print("self.tasks_buffer size", asizeof.asizeof(self.tasks_buffer))
    96                                         
    97                                                     #####
    98                                                     # Convert actions_hist, rewards_hist to np.array and flatten them out
    99                                                     # for example: hist =7, actin_dim = 11 --> np.asarray(actions_hist(7, 11)) ==> flatten ==> (77,)
   100    392.0 MiB      0.0 MiB          19               np_pre_actions = np.asarray(actions_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   101    392.0 MiB      0.0 MiB          19               np_pre_rewards = np.asarray(rewards_hist, dtype=np.float32) #(hist, )
   102    392.0 MiB      0.0 MiB          19               np_pre_obsers = np.asarray(obsvs_hist, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   103                                         
   104                                                     # Select action randomly or according to policy
   105    392.0 MiB      0.0 MiB          19               if keep_burning or update_iter < self.burn_in:
   106    392.0 MiB      0.0 MiB          19                   action = self.env.action_space.sample()
   107                                         
   108                                                     else:
   109                                                         # select_action take into account previous action to take into account
   110                                                         # previous action in selecting a new action
   111                                                         action = self.model.select_action(np.array(obs), np.array(np_pre_actions), np.array(np_pre_rewards), np.array(np_pre_obsers))
   112                                         
   113                                                         if self.expl_noise != 0: 
   114                                                             action = action + np.random.normal(0, self.expl_noise, size=self.env.action_space.shape[0])
   115                                                             action = action.clip(self.env.action_space.low, self.env.action_space.high)
   116                                         
   117                                                     # Perform action
   118    392.0 MiB      0.0 MiB          19               new_obs, reward, done, _ = self.env.step(action) 
   119    392.0 MiB      0.0 MiB          19               print("new obs size", asizeof.asizeof(new_obs))
   120    392.0 MiB      0.0 MiB          19               print("reward size", asizeof.asizeof(reward))
   121                                         
   122    392.0 MiB      0.0 MiB          19               if episode_timesteps + 1 == self.max_path_length:
   123                                                         done_bool = 0
   124                                         
   125                                                     else:
   126    392.0 MiB      0.0 MiB          19                   done_bool = float(done)
   127                                         
   128    392.0 MiB      0.0 MiB          19               episode_reward += reward
   129    392.0 MiB      0.0 MiB          19               reward_epinfos.append(reward)
   130                                         
   131                                                     ###############
   132    392.0 MiB      0.0 MiB          19               next_hrews.append(reward)
   133    392.0 MiB      0.0 MiB          19               next_hacts.append(action.copy())
   134    392.0 MiB      0.0 MiB          19               next_hobvs.append(obs.copy())
   135                                         
   136                                                     # np_next_hacts and np_next_hrews are required for TD3 alg
   137    392.0 MiB      0.0 MiB          19               np_next_hacts = np.asarray(next_hacts, dtype=np.float32).flatten()  #(hist, action_dim) => (hist *action_dim,)
   138    392.0 MiB      0.0 MiB          19               np_next_hrews = np.asarray(next_hrews, dtype=np.float32) #(hist, )
   139    392.0 MiB      0.0 MiB          19               np_next_hobvs = np.asarray(next_hobvs, dtype=np.float32).flatten() #(hist, )
   140                                         
   141                                                     # Store data in replay buffer
   142    392.0 MiB      0.0 MiB          38               self.replay_buffer.add((obs, new_obs, action, reward, done_bool,
   143    392.0 MiB      0.0 MiB          19                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   144    392.0 MiB      0.0 MiB          19                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   145                                         
   146                                                     # This is snapshot buffer which has short memeory
   147    392.0 MiB      0.0 MiB          38               self.tasks_buffer.add(task_id, (obs, new_obs, action, reward, done_bool,
   148    392.0 MiB      0.0 MiB          19                                       np_pre_actions, np_pre_rewards, np_pre_obsers,
   149    392.0 MiB      0.0 MiB          19                                       np_next_hacts, np_next_hrews, np_next_hobvs))
   150                                         
   151                                                     # new becomes old
   152    392.0 MiB      0.0 MiB          19               rewards_hist.append(reward)
   153    392.0 MiB      0.0 MiB          19               actions_hist.append(action.copy())
   154    392.0 MiB      0.0 MiB          19               obsvs_hist.append(obs.copy())
   155                                         
   156    392.0 MiB      0.0 MiB          19               obs = new_obs.copy()
   157    392.0 MiB      0.0 MiB          19               episode_timesteps += 1
   158    392.0 MiB      0.0 MiB          19               update_iter += 1
   159    392.0 MiB      0.0 MiB          19               uiter += 1
   160                                         
   161    392.0 MiB      0.0 MiB           1           info = {}
   162    392.0 MiB      0.0 MiB           1           info['episode_timesteps'] = episode_timesteps
   163    392.0 MiB      0.0 MiB           1           info['update_iter'] = update_iter
   164    392.0 MiB      0.0 MiB           1           info['episode_reward'] = episode_reward
   165    392.0 MiB      0.0 MiB           1           info['epinfos'] = [{"r": round(sum(reward_epinfos), 6), "l": len(reward_epinfos)}]
   166                                         
   167    392.0 MiB      0.0 MiB           1           return info


-----------------
env size 54448
rollouts size 5758104
args size 8664
epinfobuf size 624
epinfobuf2 size 624
:::::::::::::::::::::::
leaking objects size:  6292
obs shape (376,)
obs size 3136
obs hist size 624
self.replay_buffer size 5404328
self.tasks_buffer size 5433152
new obs size 3136
reward size 48
self.replay_buffer size 5458904
self.tasks_buffer size 5487760
new obs size 3136
reward size 48
self.replay_buffer size 5513480
self.tasks_buffer size 5542336
new obs size 3136
reward size 48
self.replay_buffer size 5568056
self.tasks_buffer size 5596912
new obs size 3136
reward size 48
self.replay_buffer size 5622632
self.tasks_buffer size 5651488
new obs size 3136
reward size 48
self.replay_buffer size 5677208
self.tasks_buffer size 5706096
new obs size 3136
reward size